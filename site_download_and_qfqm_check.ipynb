{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import thetagrids\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "\n",
    "#from dask import delayed, compute\n",
    "#from dask.diagnostics import ProgressBar\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#from dask.distributed import Client\n",
    "#c = Client()\n",
    "#c.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the dates and sites to download.  These will be used later for examining the downloaded sites as well. For the download script enter your NEON API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEON API Token\n",
    "token = '' \n",
    "\n",
    "# path where downloads will be saved upon download\n",
    "data_path = '/media/storage/NEON'\n",
    "\n",
    "# path where files we are working with will be saved\n",
    "neon_path = os.path.join('/media', 'data', 'NEON')\n",
    "\n",
    "# years we want\n",
    "years = ['2018', '2019', '2020', '2021']\n",
    "\n",
    "# sites we want\n",
    "sites = [ 'ABBY', 'BARR', 'BLAN', 'BONA', 'CLBJ', 'CPER', 'DCFS', 'DEJU',\n",
    "          'DELA', 'DSNY', 'GRSM', 'GUAN', 'HARV', 'HEAL', 'JERC', 'JORN',\n",
    "          'KONA', 'LAJA', 'LENO', 'MLBS', 'NIWO', 'NOGP', 'OAES', 'ONAQ',\n",
    "          'ORNL', 'OSBS', 'PUUM', 'RMNP', 'SERC', 'SJER', 'SOAP', 'SRER',\n",
    "          'STEI', 'STER', 'TALL', 'TEAK', 'TOOL', 'TREE', 'UKFS', 'UNDE',\n",
    "          'WOOD', 'WREF', 'YELL']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading files\n",
    "\n",
    "This section creates a shell script to dowload the files using neonUtilities.  Run the script in a terminal. It will take a long time.  This downloads the bundled eddy flux data (DP4.00200.001), then looks at the footprint and downloads the tiled hyperspectral data (DP3.30006.001) covering the footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cmds(site, year, token, data_path) :\n",
    "    '''return command for downloading data via get_flux'''\n",
    "\n",
    "    cmd = f'./start_get_flux.sh {site} {year}-04 {year}-07 $TOKEN {data_path}/{site}'\n",
    "\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of download commands\n",
    "cmds = []\n",
    "for site in tqdm(sites):\n",
    "    for year in years:\n",
    "        cmds.append(make_cmds(site, year, token, data_path))\n",
    "\n",
    "# using subprocess for this works poorly     \n",
    "#for cmd in cmds:\n",
    "#   _ = subprocess.run(cmd, shell=True, capture_output=True)\n",
    "\n",
    "\n",
    "# write a sh script to download all the files\n",
    "with open('download.sh', 'w') as dst:\n",
    "    dst.write('#!/bin/sh\\n\\n')\n",
    "    dst.write(f'TOKEN={token}\\n')\n",
    "\n",
    "    for item in cmds:\n",
    "        dst.write(f'{item}\\n')\n",
    "\n",
    "\n",
    "# go run the script in a terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Valid Observations\n",
    "\n",
    "In this section we will determine which sites have enough valid observations to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [05:47<00:00,  8.09s/it]\n"
     ]
    }
   ],
   "source": [
    "def count_valid_observations(site, files, out_path):\n",
    "    '''\n",
    "    Goes through all csv files for a site and writes a csv with\n",
    "    counts of valid observations to outpath. Valid means that\n",
    "    they exist and have a passing final QF flag.\n",
    "\n",
    "    csv has columns:\n",
    "    'site', 'CO2', 'H2O','T', 'footprint', 'all'\n",
    "\n",
    "    Each contains the numbr of valid observations for that\n",
    "    quantity. the 'all' column counts rows where all quantities\n",
    "    have valid values.\n",
    "    '''\n",
    "\n",
    "    # make empty df for quality info\n",
    "    qdf = pd.DataFrame(columns=['site',\n",
    "                                'CO2',\n",
    "                                'H2O',\n",
    "                                'T',\n",
    "                                'footprint',\n",
    "                                'all'],\n",
    "                        index=pd.to_datetime([]))\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        # get the day\n",
    "        day = pd.to_datetime(f.split('nsae.')[1].split('.')[0]).date()\n",
    "\n",
    "        # open the hdf\n",
    "        hdf = pd.HDFStore(f)\n",
    "\n",
    "        try:\n",
    "            # get the flux quality flags\n",
    "            qfqm_CO2 = hdf.get(f'{site}/dp04/qfqm/fluxCo2/nsae')\n",
    "            qfqm_H2O = hdf.get(f'{site}/dp04/qfqm/fluxH2o/nsae')\n",
    "            qfqm_T = hdf.get(f'{site}/dp04/qfqm/fluxTemp/nsae')\n",
    "            qfqm_foot = hdf.get(f'{site}/dp04/qfqm/foot/turb')\n",
    "\n",
    "            # Select observations with no bad flags\n",
    "            qfqm_CO2  = qfqm_CO2.loc[qfqm_CO2.qfFinl == 0]\n",
    "            qfqm_H2O  = qfqm_H2O.loc[qfqm_H2O.qfFinl == 0]\n",
    "            qfqm_T    = qfqm_T.loc[qfqm_T.qfFinl == 0]\n",
    "            qfqm_foot = qfqm_foot.loc[qfqm_foot.qfFinl == 0]\n",
    "\n",
    "            # get the footprint input stats\n",
    "            stat = hdf.get(f'{site}/dp04/data/foot/stat/')\n",
    "\n",
    "            # get indices of the dfs from above\n",
    "            istat  = stat.set_index('timeBgn').index\n",
    "            iqfqmC = qfqm_CO2.set_index('timeBgn').index\n",
    "            iqfqmH = qfqm_H2O.set_index('timeBgn').index\n",
    "            iqfqmT = qfqm_T.set_index('timeBgn').index\n",
    "            iqfqmf = qfqm_foot.set_index('timeBgn').index\n",
    "\n",
    "            # keep only entries in stat which correspond to good\n",
    "            # qfqm flags for all variables\n",
    "            good = stat[\n",
    "                (istat.isin(iqfqmC)) &\n",
    "                (istat.isin(iqfqmH)) &\n",
    "                (istat.isin(iqfqmT)) &\n",
    "                (istat.isin(iqfqmf))\n",
    "            ]\n",
    "\n",
    "            # make a dict of the counts for each and all\n",
    "            row = {\n",
    "                   'site': site,\n",
    "                   'CO2': len(qfqm_CO2),\n",
    "                   'H2O': len(qfqm_H2O),\n",
    "                   'T': len(qfqm_T),\n",
    "                   'footprint': len(qfqm_foot),\n",
    "                   'all': len(good)\n",
    "                   }\n",
    "\n",
    "            row = pd.DataFrame(row, index=[day])\n",
    "        \n",
    "            # add row to qdf    \n",
    "            qdf = pd.concat([qdf, row])\n",
    "\n",
    "            # close file\n",
    "            hdf.close()\n",
    "    \n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # write a copy to csv\n",
    "    qdf.to_csv(os.path.join(out_path, f'qfqm_counts_{site}.csv'))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# _____________________________________________________\n",
    "\n",
    "# path where qfqm counts will be saved\n",
    "out_path = '/media/data/NEON/all_sites'\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "for site in tqdm(sites):\n",
    "    # path to files\n",
    "    file_path = f'{data_path}/{site}/filesToStack00200'\n",
    "\n",
    "    # make list of the files for the site\n",
    "    files = [os.path.join(file_path, f)\n",
    "             for f\n",
    "             in os.listdir(file_path)\n",
    "             if '.h5' in f]\n",
    "\n",
    "    # count the valid observations\n",
    "    _ = count_valid_observations(site, files, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO2</th>\n",
       "      <th>H2O</th>\n",
       "      <th>T</th>\n",
       "      <th>footprint</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BLAN</th>\n",
       "      <td>231</td>\n",
       "      <td>228</td>\n",
       "      <td>1784</td>\n",
       "      <td>5205</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BONA</th>\n",
       "      <td>1589</td>\n",
       "      <td>1596</td>\n",
       "      <td>1987</td>\n",
       "      <td>5066</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DCFS</th>\n",
       "      <td>189</td>\n",
       "      <td>187</td>\n",
       "      <td>3020</td>\n",
       "      <td>5660</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEJU</th>\n",
       "      <td>841</td>\n",
       "      <td>800</td>\n",
       "      <td>3770</td>\n",
       "      <td>5404</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELA</th>\n",
       "      <td>1674</td>\n",
       "      <td>1638</td>\n",
       "      <td>573</td>\n",
       "      <td>5652</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HARV</th>\n",
       "      <td>768</td>\n",
       "      <td>650</td>\n",
       "      <td>3956</td>\n",
       "      <td>5053</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEAL</th>\n",
       "      <td>2985</td>\n",
       "      <td>2854</td>\n",
       "      <td>1431</td>\n",
       "      <td>5656</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KONA</th>\n",
       "      <td>2448</td>\n",
       "      <td>2551</td>\n",
       "      <td>1609</td>\n",
       "      <td>5706</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAJA</th>\n",
       "      <td>321</td>\n",
       "      <td>332</td>\n",
       "      <td>3806</td>\n",
       "      <td>4015</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LENO</th>\n",
       "      <td>800</td>\n",
       "      <td>693</td>\n",
       "      <td>4474</td>\n",
       "      <td>5316</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOGP</th>\n",
       "      <td>3104</td>\n",
       "      <td>3033</td>\n",
       "      <td>5181</td>\n",
       "      <td>5790</td>\n",
       "      <td>2537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUUM</th>\n",
       "      <td>681</td>\n",
       "      <td>595</td>\n",
       "      <td>2287</td>\n",
       "      <td>2588</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMNP</th>\n",
       "      <td>2249</td>\n",
       "      <td>2253</td>\n",
       "      <td>4386</td>\n",
       "      <td>5477</td>\n",
       "      <td>1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERC</th>\n",
       "      <td>213</td>\n",
       "      <td>210</td>\n",
       "      <td>3789</td>\n",
       "      <td>5327</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOAP</th>\n",
       "      <td>1511</td>\n",
       "      <td>1623</td>\n",
       "      <td>3303</td>\n",
       "      <td>5719</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRER</th>\n",
       "      <td>1721</td>\n",
       "      <td>1750</td>\n",
       "      <td>5255</td>\n",
       "      <td>5812</td>\n",
       "      <td>1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STEI</th>\n",
       "      <td>1764</td>\n",
       "      <td>1516</td>\n",
       "      <td>1164</td>\n",
       "      <td>5092</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STER</th>\n",
       "      <td>2215</td>\n",
       "      <td>1893</td>\n",
       "      <td>4072</td>\n",
       "      <td>5170</td>\n",
       "      <td>1431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TALL</th>\n",
       "      <td>756</td>\n",
       "      <td>732</td>\n",
       "      <td>2466</td>\n",
       "      <td>4192</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOOL</th>\n",
       "      <td>736</td>\n",
       "      <td>678</td>\n",
       "      <td>921</td>\n",
       "      <td>4985</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UKFS</th>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>4903</td>\n",
       "      <td>5319</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNDE</th>\n",
       "      <td>1078</td>\n",
       "      <td>1006</td>\n",
       "      <td>3836</td>\n",
       "      <td>5614</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOOD</th>\n",
       "      <td>947</td>\n",
       "      <td>848</td>\n",
       "      <td>4772</td>\n",
       "      <td>5470</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YELL</th>\n",
       "      <td>1538</td>\n",
       "      <td>1600</td>\n",
       "      <td>4378</td>\n",
       "      <td>5302</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CO2   H2O     T  footprint   all\n",
       "site                                   \n",
       "BLAN   231   228  1784       5205   180\n",
       "BONA  1589  1596  1987       5066   501\n",
       "DCFS   189   187  3020       5660   166\n",
       "DEJU   841   800  3770       5404   715\n",
       "DELA  1674  1638   573       5652   319\n",
       "HARV   768   650  3956       5053   567\n",
       "HEAL  2985  2854  1431       5656   875\n",
       "KONA  2448  2551  1609       5706   651\n",
       "LAJA   321   332  3806       4015   283\n",
       "LENO   800   693  4474       5316   579\n",
       "NOGP  3104  3033  5181       5790  2537\n",
       "PUUM   681   595  2287       2588   534\n",
       "RMNP  2249  2253  4386       5477  1750\n",
       "SERC   213   210  3789       5327   154\n",
       "SOAP  1511  1623  3303       5719   851\n",
       "SRER  1721  1750  5255       5812  1460\n",
       "STEI  1764  1516  1164       5092   332\n",
       "STER  2215  1893  4072       5170  1431\n",
       "TALL   756   732  2466       4192   613\n",
       "TOOL   736   678   921       4985   144\n",
       "UKFS   199   199  4903       5319   199\n",
       "UNDE  1078  1006  3836       5614   843\n",
       "WOOD   947   848  4772       5470   763\n",
       "YELL  1538  1600  4378       5302  1196"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make list of files in the dir where the csvs were written\n",
    "files = [os.path.join(out_path,f)\n",
    "         for f\n",
    "         in os.listdir(out_path)\n",
    "         if f.endswith('.csv')]\n",
    "\n",
    "# read all the csvs into df         \n",
    "qdf = pd.concat((pd.read_csv(f)) for f in files)\n",
    "\n",
    "# group by site\n",
    "sums = qdf.groupby('site').sum()\n",
    "\n",
    "# filter for sites with more than 100 valid observations\n",
    "sums = sums.loc[sums['all'] > 100]\n",
    "\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17643"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find total valid 'all' observations in the the selected sites\n",
    "sums['all'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of selected sites\n",
    "len(sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating csv files of valid observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_observations(site, files):\n",
    "    '''\n",
    "    Goes through all csv files for a site returns a df of valid\n",
    "    observations. Valid means that they exist and have a\n",
    "    passing final QF flag.\n",
    "    '''\n",
    "\n",
    "    # make empty list for dfs\n",
    "    dfs = []\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        # get the day\n",
    "        day = pd.to_datetime(f.split('nsae.')[1].split('.')[0]).date()\n",
    "\n",
    "        # open the hdf\n",
    "        hdf = pd.HDFStore(f)\n",
    "\n",
    "        try:\n",
    "            # get the flux quality flags\n",
    "            qfqm_CO2 = hdf.get(f'{site}/dp04/qfqm/fluxCo2/nsae')\n",
    "            qfqm_H2O = hdf.get(f'{site}/dp04/qfqm/fluxH2o/nsae')\n",
    "            qfqm_T = hdf.get(f'{site}/dp04/qfqm/fluxTemp/nsae')\n",
    "            qfqm_foot = hdf.get(f'{site}/dp04/qfqm/foot/turb')\n",
    "\n",
    "            # Select observations with no bad flags\n",
    "            qfqm_CO2  = qfqm_CO2.loc[qfqm_CO2.qfFinl == 0]\n",
    "            qfqm_H2O  = qfqm_H2O.loc[qfqm_H2O.qfFinl == 0]\n",
    "            qfqm_T    = qfqm_T.loc[qfqm_T.qfFinl == 0]\n",
    "            qfqm_foot = qfqm_foot.loc[qfqm_foot.qfFinl == 0]\n",
    "\n",
    "            # get the footprint input stats\n",
    "            stat = hdf.get(f'{site}/dp04/data/foot/stat/')\n",
    "\n",
    "            # get indices of the dfs from above\n",
    "            istat  = stat.set_index('timeBgn').index\n",
    "            iqfqmC = qfqm_CO2.set_index('timeBgn').index\n",
    "            iqfqmH = qfqm_H2O.set_index('timeBgn').index\n",
    "            iqfqmT = qfqm_T.set_index('timeBgn').index\n",
    "            iqfqmf = qfqm_foot.set_index('timeBgn').index\n",
    "\n",
    "            # keep only entries in stat which correspond to good\n",
    "            # qfqm flags for all variables\n",
    "            stat = stat[\n",
    "                (istat.isin(iqfqmC)) &\n",
    "                (istat.isin(iqfqmH)) &\n",
    "                (istat.isin(iqfqmT)) &\n",
    "                (istat.isin(iqfqmf))\n",
    "            ]\n",
    "\n",
    "            # get the flux data\n",
    "            fluxCo2 = hdf.get(f'{site}/dp04/data/fluxCo2/nsae').drop('timeEnd', axis=1)\n",
    "            fluxH2o = hdf.get(f'{site}/dp04/data/fluxH2o/nsae').drop('timeEnd', axis=1)\n",
    "            fluxTemp = hdf.get(f'{site}/dp04/data/fluxTemp/nsae').drop('timeEnd', axis=1)\n",
    "\n",
    "            # now merge dfs onto stat\n",
    "            stat = stat.merge(fluxCo2, how='left', on='timeBgn', suffixes=('_stat', ''))\n",
    "            stat = stat.merge(fluxH2o, how='left', on='timeBgn', suffixes=('_CO2', ''))\n",
    "            stat = stat.merge(fluxTemp, how='left', on='timeBgn', suffixes=('_H20', '_T'))\n",
    "\n",
    "            dfs.append(stat)\n",
    "\n",
    "            # close file\n",
    "            hdf.close()\n",
    "    \n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [05:12<00:00, 13.01s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sites = list(sums.index)\n",
    "\n",
    "for site in tqdm(sites):\n",
    "    # make sure site dir exists, make filename\n",
    "    site_path = os.path.join(neon_path, site)\n",
    "    csv_path = os.path.join(site_path, f'flux_data.csv')\n",
    "    os.makedirs(site_path, exist_ok=True)\n",
    "\n",
    "    # path to files\n",
    "    file_path = f'{data_path}/{site}/filesToStack00200'\n",
    "\n",
    "    # make list of the files for the site\n",
    "    files = [\n",
    "            os.path.join(file_path, f)\n",
    "            for f\n",
    "            in os.listdir(file_path)\n",
    "            if ('.h5' in f)\n",
    "            ]\n",
    "\n",
    "\n",
    "    df = get_valid_observations(site, files)\n",
    "    df.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating  stratified sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sectors(df, theta=10):\n",
    "    '''\n",
    "    Adds sector column to df\n",
    "    '''\n",
    "\n",
    "    # make sure theta goes into 360 an even number of times\n",
    "    if 360 % theta != 0:\n",
    "        while 360 % theta != 0:\n",
    "            theta= theta + 1\n",
    "        print(f'theta has been forced to {theta} for even division of 360')\n",
    "\n",
    "    # set start angle, and empy list\n",
    "    df['sector'] = theta * (df.angZaxsErth // theta)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees per sector\n",
    "θ = 18\n",
    "\n",
    "# make empty df\n",
    "sectors = pd.DataFrame(columns=['timeBgn', 'sector'])\n",
    "\n",
    "for site in sites[:1]:\n",
    "    csv_path = os.path.join(neon_path, site, 'flux_data.csv')\n",
    "    flux_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # find sectors in which observations lie\n",
    "    flux_df = find_sectors(flux_df, theta=θ)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeEnd</th>\n",
       "      <th>angZaxsErth</th>\n",
       "      <th>distReso</th>\n",
       "      <th>veloYaxsHorSd</th>\n",
       "      <th>veloZaxsHorSd</th>\n",
       "      <th>veloFric</th>\n",
       "      <th>distZaxsMeasDisp</th>\n",
       "      <th>distZaxsRgh</th>\n",
       "      <th>distZaxsAbl</th>\n",
       "      <th>distXaxs90</th>\n",
       "      <th>distXaxsMax</th>\n",
       "      <th>distYaxs90</th>\n",
       "      <th>flux_CO2</th>\n",
       "      <th>flux_H20</th>\n",
       "      <th>timeBgn</th>\n",
       "      <th>flux_T</th>\n",
       "      <th>sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-08T00:59:59.950Z</td>\n",
       "      <td>345.043925</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.627431</td>\n",
       "      <td>0.297594</td>\n",
       "      <td>0.227588</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>152.16</td>\n",
       "      <td>63.40</td>\n",
       "      <td>31.70</td>\n",
       "      <td>4.345192</td>\n",
       "      <td>-3.036561</td>\n",
       "      <td>2018-04-08T00:30:00.000Z</td>\n",
       "      <td>-34.410926</td>\n",
       "      <td>342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-08T01:29:59.950Z</td>\n",
       "      <td>357.408726</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.395602</td>\n",
       "      <td>0.245316</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>158.50</td>\n",
       "      <td>69.74</td>\n",
       "      <td>25.36</td>\n",
       "      <td>1.491616</td>\n",
       "      <td>1.493820</td>\n",
       "      <td>2018-04-08T01:00:00.000Z</td>\n",
       "      <td>-26.646935</td>\n",
       "      <td>342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-08T01:59:59.950Z</td>\n",
       "      <td>351.889928</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.245366</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.485931</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>177.52</td>\n",
       "      <td>76.08</td>\n",
       "      <td>19.02</td>\n",
       "      <td>1.988864</td>\n",
       "      <td>16.345876</td>\n",
       "      <td>2018-04-08T01:30:00.000Z</td>\n",
       "      <td>-15.238791</td>\n",
       "      <td>342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-08T03:59:59.950Z</td>\n",
       "      <td>347.133152</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.532786</td>\n",
       "      <td>0.366245</td>\n",
       "      <td>0.237374</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.564311</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>139.48</td>\n",
       "      <td>57.06</td>\n",
       "      <td>19.02</td>\n",
       "      <td>3.452583</td>\n",
       "      <td>6.248057</td>\n",
       "      <td>2018-04-08T03:30:00.000Z</td>\n",
       "      <td>-50.946447</td>\n",
       "      <td>342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-08T04:29:59.950Z</td>\n",
       "      <td>350.910019</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.510539</td>\n",
       "      <td>0.334296</td>\n",
       "      <td>0.235507</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.468437</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>152.16</td>\n",
       "      <td>63.40</td>\n",
       "      <td>19.02</td>\n",
       "      <td>1.694864</td>\n",
       "      <td>5.221214</td>\n",
       "      <td>2018-04-08T04:00:00.000Z</td>\n",
       "      <td>-33.512011</td>\n",
       "      <td>342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2018-04-03T19:29:59.950Z</td>\n",
       "      <td>64.203144</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.944606</td>\n",
       "      <td>0.243230</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>158.50</td>\n",
       "      <td>69.74</td>\n",
       "      <td>57.06</td>\n",
       "      <td>0.250161</td>\n",
       "      <td>-7.060549</td>\n",
       "      <td>2018-04-03T19:00:00.000Z</td>\n",
       "      <td>29.907526</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2018-04-03T19:59:59.950Z</td>\n",
       "      <td>39.960324</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.731510</td>\n",
       "      <td>0.268563</td>\n",
       "      <td>0.208439</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.364205</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>177.52</td>\n",
       "      <td>69.74</td>\n",
       "      <td>31.70</td>\n",
       "      <td>-0.223327</td>\n",
       "      <td>1.635765</td>\n",
       "      <td>2018-04-03T19:30:00.000Z</td>\n",
       "      <td>39.869987</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2018-04-03T20:59:59.950Z</td>\n",
       "      <td>182.123728</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.749050</td>\n",
       "      <td>0.481824</td>\n",
       "      <td>0.421962</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>171.18</td>\n",
       "      <td>69.74</td>\n",
       "      <td>25.36</td>\n",
       "      <td>2.467898</td>\n",
       "      <td>3.776534</td>\n",
       "      <td>2018-04-03T20:30:00.000Z</td>\n",
       "      <td>90.947143</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2018-04-03T22:29:59.950Z</td>\n",
       "      <td>144.742877</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.514034</td>\n",
       "      <td>0.364015</td>\n",
       "      <td>0.294367</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.423205</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>177.52</td>\n",
       "      <td>69.74</td>\n",
       "      <td>19.02</td>\n",
       "      <td>1.448730</td>\n",
       "      <td>2.351311</td>\n",
       "      <td>2018-04-03T22:00:00.000Z</td>\n",
       "      <td>10.831379</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2018-04-03T23:29:59.950Z</td>\n",
       "      <td>130.044097</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.781670</td>\n",
       "      <td>0.347949</td>\n",
       "      <td>0.305027</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.500896</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>177.52</td>\n",
       "      <td>76.08</td>\n",
       "      <td>31.70</td>\n",
       "      <td>1.901952</td>\n",
       "      <td>-0.270590</td>\n",
       "      <td>2018-04-03T23:00:00.000Z</td>\n",
       "      <td>-15.049284</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      timeEnd  angZaxsErth  distReso  veloYaxsHorSd  \\\n",
       "0    2018-04-08T00:59:59.950Z   345.043925      6.34       0.627431   \n",
       "1    2018-04-08T01:29:59.950Z   357.408726      6.34       0.395602   \n",
       "2    2018-04-08T01:59:59.950Z   351.889928      6.34       0.245366   \n",
       "3    2018-04-08T03:59:59.950Z   347.133152      6.34       0.532786   \n",
       "4    2018-04-08T04:29:59.950Z   350.910019      6.34       0.510539   \n",
       "..                        ...          ...       ...            ...   \n",
       "175  2018-04-03T19:29:59.950Z    64.203144      6.34       0.944606   \n",
       "176  2018-04-03T19:59:59.950Z    39.960324      6.34       0.731510   \n",
       "177  2018-04-03T20:59:59.950Z   182.123728      6.34       0.749050   \n",
       "178  2018-04-03T22:29:59.950Z   144.742877      6.34       0.514034   \n",
       "179  2018-04-03T23:29:59.950Z   130.044097      6.34       0.781670   \n",
       "\n",
       "     veloZaxsHorSd  veloFric  distZaxsMeasDisp  distZaxsRgh  distZaxsAbl  \\\n",
       "0         0.297594  0.227588              6.34     0.634000       1000.0   \n",
       "1         0.245316  0.200000              6.34     0.634000       1000.0   \n",
       "2         0.230000  0.200000              6.34     0.485931       1000.0   \n",
       "3         0.366245  0.237374              6.34     0.564311       1000.0   \n",
       "4         0.334296  0.235507              6.34     0.468437       1000.0   \n",
       "..             ...       ...               ...          ...          ...   \n",
       "175       0.243230  0.200000              6.34     0.634000       1000.0   \n",
       "176       0.268563  0.208439              6.34     0.364205       1000.0   \n",
       "177       0.481824  0.421962              6.34     0.634000       1000.0   \n",
       "178       0.364015  0.294367              6.34     0.423205       1000.0   \n",
       "179       0.347949  0.305027              6.34     0.500896       1000.0   \n",
       "\n",
       "     distXaxs90  distXaxsMax  distYaxs90  flux_CO2   flux_H20  \\\n",
       "0        152.16        63.40       31.70  4.345192  -3.036561   \n",
       "1        158.50        69.74       25.36  1.491616   1.493820   \n",
       "2        177.52        76.08       19.02  1.988864  16.345876   \n",
       "3        139.48        57.06       19.02  3.452583   6.248057   \n",
       "4        152.16        63.40       19.02  1.694864   5.221214   \n",
       "..          ...          ...         ...       ...        ...   \n",
       "175      158.50        69.74       57.06  0.250161  -7.060549   \n",
       "176      177.52        69.74       31.70 -0.223327   1.635765   \n",
       "177      171.18        69.74       25.36  2.467898   3.776534   \n",
       "178      177.52        69.74       19.02  1.448730   2.351311   \n",
       "179      177.52        76.08       31.70  1.901952  -0.270590   \n",
       "\n",
       "                      timeBgn     flux_T  sector  \n",
       "0    2018-04-08T00:30:00.000Z -34.410926   342.0  \n",
       "1    2018-04-08T01:00:00.000Z -26.646935   342.0  \n",
       "2    2018-04-08T01:30:00.000Z -15.238791   342.0  \n",
       "3    2018-04-08T03:30:00.000Z -50.946447   342.0  \n",
       "4    2018-04-08T04:00:00.000Z -33.512011   342.0  \n",
       "..                        ...        ...     ...  \n",
       "175  2018-04-03T19:00:00.000Z  29.907526    54.0  \n",
       "176  2018-04-03T19:30:00.000Z  39.869987    36.0  \n",
       "177  2018-04-03T20:30:00.000Z  90.947143   180.0  \n",
       "178  2018-04-03T22:00:00.000Z  10.831379   144.0  \n",
       "179  2018-04-03T23:00:00.000Z -15.049284   126.0  \n",
       "\n",
       "[180 rows x 17 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flux_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_cluster(df,k_range=(2, 10), verbose=True):\n",
    "    '''\n",
    "    Performs Gaussian Mixture Model clustering.\n",
    "    df - dataframe containing only the features to\n",
    "         be used in clustering\n",
    "    k_range - tuple, representing range of numbers\n",
    "         of clusters to be tried.\n",
    "    '''\n",
    "    # empty list\n",
    "    silhouette_scores  = []\n",
    "\n",
    "    # make list of range\n",
    "    k_r = list(range(*k_range))\n",
    "\n",
    "    for k in k_r:\n",
    "        # make model\n",
    "        model = GaussianMixture(n_components=k,\n",
    "                                n_init=10,\n",
    "                                init_params='kmeans')\n",
    "        # fit\n",
    "        labels = model.fit_predict(df)\n",
    "\n",
    "        # calculate silhouette score\n",
    "        silhouette_scores.append(\n",
    "            metrics.silhouette_score(df, labels, metric='euclidean')\n",
    "            )\n",
    "\n",
    "    # choose the highest silhouette scoring k\n",
    "    k = k_r[np.argmax(silhouette_scores)]\n",
    "\n",
    "    # now create full model\n",
    "    modelk = GaussianMixture(\n",
    "        n_components=k,\n",
    "        covariance_type='full',\n",
    "        random_state=1\n",
    "        )\n",
    "\n",
    "    cluster = modelk.fit(df)\n",
    "    labels = modelk.predict(df)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'*************** {k} Cluster Model ***************')\n",
    "        print('Weights: ', cluster.weights_)\n",
    "        print('Converged: ', cluster.converged_)\n",
    "        print('No. of Iterations: ', cluster.n_iter_)\n",
    "        print('Lower Bound: ', cluster.lower_bound_)\n",
    "        print(f'*************************************************')\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** 8 Cluster Model ***************\n",
      "Weights:  [0.34415242 0.17224005 0.00555556 0.36111012 0.00555556 0.01666667\n",
      " 0.0333155  0.06140413]\n",
      "Converged:  True\n",
      "No. of Iterations:  12\n",
      "Lower Bound:  12.81607213852765\n",
      "*************************************************\n"
     ]
    }
   ],
   "source": [
    "# columns to use in clustering\n",
    "cols = ['veloYaxsHorSd',\n",
    "'veloZaxsHorSd',\n",
    "'veloFric',\n",
    "'distZaxsMeasDisp',\n",
    "'distZaxsRgh',\n",
    "'distZaxsAbl',\n",
    "'distXaxs90',\n",
    "'distXaxsMax', \n",
    "'distYaxs90']\n",
    "\n",
    "flux_df['label'] = gmm_cluster(flux_df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [04:02<00:00, 10.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for site in tqdm(sites):\n",
    "    src_dir = os.path.join(data_path, site, 'footprints')\n",
    "    src_files = [os.path.join(src_dir, f)\n",
    "                for f in os.listdir(src_dir)\n",
    "                if f.endswith('.tiff')]\n",
    "\n",
    "    dst = os.path.join(neon_path, site, 'footprints')\n",
    "    os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "    \n",
    "    for src in src_files:\n",
    "        shutil.copy(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/data/NEON'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neon_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/storage/NEON'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b20d2e798ab320dcd9e3022b3f23321d95437ce0beeb6a34c6f05479d8e44a2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('geo3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
