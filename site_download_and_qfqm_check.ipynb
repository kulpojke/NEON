{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import thetagrids\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from dask import delayed, compute\n",
    "#from dask.diagnostics import ProgressBar\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#from dask.distributed import Client\n",
    "#c = Client()\n",
    "#c.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the dates and sites to download.  These will be used later for examining the downloaded sites as well. For the download script enter your NEON API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEON API Token\n",
    "token = '' \n",
    "\n",
    "# pathe where downloads will be saved\n",
    "data_path = '/media/storage/NEON'\n",
    "\n",
    "# years we want\n",
    "years = ['2018', '2019', '2020', '2021']\n",
    "\n",
    "# sites we want\n",
    "sites = [ 'ABBY', 'BARR', 'BLAN', 'BONA', 'CLBJ', 'CPER', 'DCFS', 'DEJU',\n",
    "          'DELA', 'DSNY', 'GRSM', 'GUAN', 'HARV', 'HEAL', 'JERC', 'JORN',\n",
    "          'KONA', 'LAJA', 'LENO', 'MLBS', 'NIWO', 'NOGP', 'OAES', 'ONAQ',\n",
    "          'ORNL', 'OSBS', 'PUUM', 'RMNP', 'SERC', 'SJER', 'SOAP', 'SRER',\n",
    "          'STEI', 'STER', 'TALL', 'TEAK', 'TOOL', 'TREE', 'UKFS', 'UNDE',\n",
    "          'WOOD', 'WREF', 'YELL']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading files\n",
    "\n",
    "This section creates a shell script to dowload the files using neonUtilities.  Run the script in a terminal. It will take a long time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cmds(site, year, token, data_path) :\n",
    "    '''return command for downloading data via get_flux'''\n",
    "\n",
    "    cmd = f'./start_get_flux.sh {site} {year}-04 {year}-07 $TOKEN {data_path}/{site}'\n",
    "\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of download commands\n",
    "cmds = []\n",
    "for site in tqdm(sites):\n",
    "    for year in years:\n",
    "        cmds.append(make_cmds(site, year, token, data_path))\n",
    "\n",
    "# using subprocess for this works poorly     \n",
    "#for cmd in cmds:\n",
    "#   _ = subprocess.run(cmd, shell=True, capture_output=True)\n",
    "\n",
    "\n",
    "# write a sh script to download all the files\n",
    "with open('download.sh', 'w') as dst:\n",
    "    dst.write('#!/bin/sh\\n\\n')\n",
    "    dst.write(f'TOKEN={token}\\n')\n",
    "\n",
    "    for item in cmds:\n",
    "        dst.write(f'{item}\\n')\n",
    "\n",
    "\n",
    "# go run the script in a terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Valid Observations\n",
    "\n",
    "In this section we will determine which sites have enough valid observations to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 13/43 [01:44<03:34,  7.16s/it]"
     ]
    }
   ],
   "source": [
    "def count_valid_observations(site, files, out_path):\n",
    "    '''\n",
    "    Goes through all csv files for a site and writes a csv with\n",
    "    counts of valid observations to outpath. Valid means that\n",
    "    they exist and have a passing final QF flag.\n",
    "\n",
    "    csv has columns:\n",
    "    'site', 'CO2', 'H2O','T', 'footprint', 'all'\n",
    "\n",
    "    Each contains the numbr of valid observations for that\n",
    "    quantity. the 'all' column counts rows where all quantities\n",
    "    have valid values.\n",
    "    '''\n",
    "\n",
    "    # make empty df for quality info\n",
    "    qdf = pd.DataFrame(columns=['site',\n",
    "                                'CO2',\n",
    "                                'H2O',\n",
    "                                'T',\n",
    "                                'footprint',\n",
    "                                'all'],\n",
    "                        index=pd.to_datetime([]))\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        # get the day\n",
    "        day = pd.to_datetime(f.split('nsae.')[1].split('.')[0]).date()\n",
    "\n",
    "        # open the hdf\n",
    "        hdf = pd.HDFStore(f)\n",
    "\n",
    "        try:\n",
    "            # get the flux quality flags\n",
    "            qfqm_CO2 = hdf.get(f'{site}/dp04/qfqm/fluxCo2/nsae')\n",
    "            qfqm_H2O = hdf.get(f'{site}/dp04/qfqm/fluxH2o/nsae')\n",
    "            qfqm_T = hdf.get(f'{site}/dp04/qfqm/fluxTemp/nsae')\n",
    "            qfqm_foot = hdf.get(f'{site}/dp04/qfqm/foot/turb')\n",
    "\n",
    "            # Select observations with no bad flags\n",
    "            qfqm_CO2  = qfqm_CO2.loc[qfqm_CO2.qfFinl == 0]\n",
    "            qfqm_H2O  = qfqm_H2O.loc[qfqm_H2O.qfFinl == 0]\n",
    "            qfqm_T    = qfqm_T.loc[qfqm_T.qfFinl == 0]\n",
    "            qfqm_foot = qfqm_foot.loc[qfqm_foot.qfFinl == 0]\n",
    "\n",
    "            # get the footprint input stats\n",
    "            stat = hdf.get(f'{site}/dp04/data/foot/stat/')\n",
    "\n",
    "            # get indices of the dfs from above\n",
    "            istat  = stat.set_index('timeBgn').index\n",
    "            iqfqmC = qfqm_CO2.set_index('timeBgn').index\n",
    "            iqfqmH = qfqm_H2O.set_index('timeBgn').index\n",
    "            iqfqmT = qfqm_T.set_index('timeBgn').index\n",
    "            iqfqmf = qfqm_foot.set_index('timeBgn').index\n",
    "\n",
    "            # keep only entries in stat which correspond to good\n",
    "            # qfqm flags for all variables\n",
    "            good = stat[\n",
    "                (istat.isin(iqfqmC)) &\n",
    "                (istat.isin(iqfqmH)) &\n",
    "                (istat.isin(iqfqmT)) &\n",
    "                (istat.isin(iqfqmf))\n",
    "            ]\n",
    "\n",
    "            # make a dict of the counts for each and all\n",
    "            row = {\n",
    "                   'site': site,\n",
    "                   'CO2': len(qfqm_CO2),\n",
    "                   'H2O': len(qfqm_H2O),\n",
    "                   'T': len(qfqm_T),\n",
    "                   'footprint': len(qfqm_foot),\n",
    "                   'all': len(good)\n",
    "                   }\n",
    "\n",
    "            row = pd.DataFrame(row, index=[day])\n",
    "        \n",
    "            # add row to qdf    \n",
    "            qdf = pd.concat([qdf, row])\n",
    "\n",
    "            # close file\n",
    "            hdf.close()\n",
    "    \n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # write a copy to csv\n",
    "    qdf.to_csv(os.path.join(out_path, f'qfqm_counts_{site}.csv'))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# _____________________________________________________\n",
    "\n",
    "# path where qfqm counts will be saved\n",
    "out_path = '/media/data/NEON/all_sites'\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "for site in tqdm(sites):\n",
    "    # path to files\n",
    "    file_path = f'{data_path}/{site}/filesToStack00200'\n",
    "\n",
    "    # make list of the files for the site\n",
    "    files = [os.path.join(file_path, f)\n",
    "             for f\n",
    "             in os.listdir(file_path)\n",
    "             if '.h5' in f]\n",
    "\n",
    "    # count the valid observations\n",
    "    _ = count_valid_observations(site, files, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO2</th>\n",
       "      <th>H2O</th>\n",
       "      <th>T</th>\n",
       "      <th>footprint</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BLAN</th>\n",
       "      <td>231</td>\n",
       "      <td>228</td>\n",
       "      <td>1784</td>\n",
       "      <td>5205</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BONA</th>\n",
       "      <td>1589</td>\n",
       "      <td>1596</td>\n",
       "      <td>1987</td>\n",
       "      <td>5066</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DCFS</th>\n",
       "      <td>189</td>\n",
       "      <td>187</td>\n",
       "      <td>3020</td>\n",
       "      <td>5660</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEJU</th>\n",
       "      <td>841</td>\n",
       "      <td>800</td>\n",
       "      <td>3770</td>\n",
       "      <td>5404</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELA</th>\n",
       "      <td>1674</td>\n",
       "      <td>1638</td>\n",
       "      <td>573</td>\n",
       "      <td>5652</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HARV</th>\n",
       "      <td>768</td>\n",
       "      <td>650</td>\n",
       "      <td>3956</td>\n",
       "      <td>5053</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEAL</th>\n",
       "      <td>2985</td>\n",
       "      <td>2854</td>\n",
       "      <td>1431</td>\n",
       "      <td>5656</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KONA</th>\n",
       "      <td>2448</td>\n",
       "      <td>2551</td>\n",
       "      <td>1609</td>\n",
       "      <td>5706</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAJA</th>\n",
       "      <td>321</td>\n",
       "      <td>332</td>\n",
       "      <td>3806</td>\n",
       "      <td>4015</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LENO</th>\n",
       "      <td>800</td>\n",
       "      <td>693</td>\n",
       "      <td>4474</td>\n",
       "      <td>5316</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOGP</th>\n",
       "      <td>3104</td>\n",
       "      <td>3033</td>\n",
       "      <td>5181</td>\n",
       "      <td>5790</td>\n",
       "      <td>2537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUUM</th>\n",
       "      <td>681</td>\n",
       "      <td>595</td>\n",
       "      <td>2287</td>\n",
       "      <td>2588</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMNP</th>\n",
       "      <td>2249</td>\n",
       "      <td>2253</td>\n",
       "      <td>4386</td>\n",
       "      <td>5477</td>\n",
       "      <td>1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERC</th>\n",
       "      <td>213</td>\n",
       "      <td>210</td>\n",
       "      <td>3789</td>\n",
       "      <td>5327</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOAP</th>\n",
       "      <td>1511</td>\n",
       "      <td>1623</td>\n",
       "      <td>3303</td>\n",
       "      <td>5719</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRER</th>\n",
       "      <td>1721</td>\n",
       "      <td>1750</td>\n",
       "      <td>5255</td>\n",
       "      <td>5812</td>\n",
       "      <td>1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STEI</th>\n",
       "      <td>1764</td>\n",
       "      <td>1516</td>\n",
       "      <td>1164</td>\n",
       "      <td>5092</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STER</th>\n",
       "      <td>2215</td>\n",
       "      <td>1893</td>\n",
       "      <td>4072</td>\n",
       "      <td>5170</td>\n",
       "      <td>1431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TALL</th>\n",
       "      <td>756</td>\n",
       "      <td>732</td>\n",
       "      <td>2466</td>\n",
       "      <td>4192</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOOL</th>\n",
       "      <td>736</td>\n",
       "      <td>678</td>\n",
       "      <td>921</td>\n",
       "      <td>4985</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UKFS</th>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>4903</td>\n",
       "      <td>5319</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNDE</th>\n",
       "      <td>1078</td>\n",
       "      <td>1006</td>\n",
       "      <td>3836</td>\n",
       "      <td>5614</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOOD</th>\n",
       "      <td>947</td>\n",
       "      <td>848</td>\n",
       "      <td>4772</td>\n",
       "      <td>5470</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YELL</th>\n",
       "      <td>1538</td>\n",
       "      <td>1600</td>\n",
       "      <td>4378</td>\n",
       "      <td>5302</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CO2   H2O     T  footprint   all\n",
       "site                                   \n",
       "BLAN   231   228  1784       5205   180\n",
       "BONA  1589  1596  1987       5066   501\n",
       "DCFS   189   187  3020       5660   166\n",
       "DEJU   841   800  3770       5404   715\n",
       "DELA  1674  1638   573       5652   319\n",
       "HARV   768   650  3956       5053   567\n",
       "HEAL  2985  2854  1431       5656   875\n",
       "KONA  2448  2551  1609       5706   651\n",
       "LAJA   321   332  3806       4015   283\n",
       "LENO   800   693  4474       5316   579\n",
       "NOGP  3104  3033  5181       5790  2537\n",
       "PUUM   681   595  2287       2588   534\n",
       "RMNP  2249  2253  4386       5477  1750\n",
       "SERC   213   210  3789       5327   154\n",
       "SOAP  1511  1623  3303       5719   851\n",
       "SRER  1721  1750  5255       5812  1460\n",
       "STEI  1764  1516  1164       5092   332\n",
       "STER  2215  1893  4072       5170  1431\n",
       "TALL   756   732  2466       4192   613\n",
       "TOOL   736   678   921       4985   144\n",
       "UKFS   199   199  4903       5319   199\n",
       "UNDE  1078  1006  3836       5614   843\n",
       "WOOD   947   848  4772       5470   763\n",
       "YELL  1538  1600  4378       5302  1196"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make list of files in the dir whaere the csvs were written\n",
    "files = [os.path.join(out_path,f)\n",
    "         for f\n",
    "         in os.listdir(out_path)\n",
    "         if f.endswith('.csv')]\n",
    "\n",
    "# read all the csvs into df         \n",
    "qdf = pd.concat((pd.read_csv(f)) for f in files)\n",
    "\n",
    "# group by site\n",
    "sums = qdf.groupby('site').sum()\n",
    "\n",
    "# filter for sites with more than 100 valid observations\n",
    "sums = sums.loc[sums['all'] > 100]\n",
    "\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17643"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums['all'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sums)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b20d2e798ab320dcd9e3022b3f23321d95437ce0beeb6a34c6f05479d8e44a2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('geo3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
